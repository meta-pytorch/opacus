{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Controller-Based Privacy Engine (No Model Wrapping)\n",
    "\n",
    "This tutorial demonstrates how to use Opacus's controller-based privacy engine (`PrivacyEngineGradSampleController`), which provides better compatibility with transformer models and other complex architectures by **avoiding model wrapping**.\n",
    "\n",
    "## Why Controller-Based?\n",
    "\n",
    "The standard `PrivacyEngine` wraps your model in a `GradSampleModule`, which can cause issues with:\n",
    "- **Type checking**: `isinstance()` checks fail because the model is wrapped\n",
    "- **State dict compatibility**: Wrapped models have `_module.` prefixes that complicate checkpoint loading\n",
    "- **Complex architectures**: Models with custom `__getattr__` logic (e.g., HuggingFace transformers)\n",
    "\n",
    "The controller-based approach attaches hooks directly to your model via a `GradSampleController` **without wrapping it**, keeping your model's type and structure intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and create a simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opacus.privacy_engine_gsc import PrivacyEngineGradSampleController\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 10\n",
    "\n",
    "X = torch.randn(n_samples, n_features)\n",
    "y = torch.randint(0, n_classes, (n_samples,))\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Model\n",
    "\n",
    "Let's create a simple neural network classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleClassifier(n_features, 64, n_classes)\n",
    "print(f\"Model type before: {type(model).__name__}\")\n",
    "print(f\"isinstance check before: {isinstance(model, SimpleClassifier)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard PrivacyEngine (for comparison)\n",
    "\n",
    "Let's first see what happens with the standard `PrivacyEngine`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "# Create a fresh model for standard approach\n",
    "model_standard = SimpleClassifier(n_features, 64, n_classes)\n",
    "optimizer_standard = optim.Adam(model_standard.parameters(), lr=0.001)\n",
    "\n",
    "privacy_engine_standard = PrivacyEngine()\n",
    "model_standard, optimizer_standard, dataloader_standard = privacy_engine_standard.make_private(\n",
    "    module=model_standard,\n",
    "    optimizer=optimizer_standard,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "print(f\"\\nStandard PrivacyEngine:\")\n",
    "print(f\"Model type after: {type(model_standard).__name__}\")\n",
    "print(f\"isinstance check after: {isinstance(model_standard, SimpleClassifier)}\")\n",
    "print(f\"State dict keys (first 3): {list(model_standard.state_dict().keys())[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model is now wrapped in `GradSampleModule`, `isinstance` checks fail, and state dict keys have `_module.` prefixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller-Based PrivacyEngine\n",
    "\n",
    "Now let's use the controller-based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh model for controller-based approach\n",
    "model = SimpleClassifier(n_features, 64, n_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize controller-based privacy engine\n",
    "privacy_engine = PrivacyEngineGradSampleController()\n",
    "\n",
    "model, optimizer, dataloader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "print(f\"\\nController-Based PrivacyEngine:\")\n",
    "print(f\"Model type after: {type(model).__name__}\")\n",
    "print(f\"isinstance check after: {isinstance(model, SimpleClassifier)}\")\n",
    "print(f\"State dict keys (first 3): {list(model.state_dict().keys())[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model **keeps its original type**, `isinstance` checks **still work**, and state dict keys are **clean without prefixes**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop is identical to standard PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 3\n",
    "DELTA = 1e-5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Loss: {avg_loss:.4f} | ε: {epsilon:.2f} (δ={DELTA})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `make_private_with_epsilon`\n",
    "\n",
    "You can also specify a target epsilon and have the privacy engine compute the appropriate noise multiplier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh instances\n",
    "model2 = SimpleClassifier(n_features, 64, n_classes)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "dataloader2 = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "privacy_engine2 = PrivacyEngineGradSampleController()\n",
    "\n",
    "model2, optimizer2, dataloader2 = privacy_engine2.make_private_with_epsilon(\n",
    "    module=model2,\n",
    "    optimizer=optimizer2,\n",
    "    data_loader=dataloader2,\n",
    "    target_epsilon=3.0,\n",
    "    target_delta=1e-5,\n",
    "    epochs=EPOCHS,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "print(f\"Target epsilon: 3.0\")\n",
    "print(f\"Computed noise multiplier: {privacy_engine2.noise_multiplier:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Saving and Loading\n",
    "\n",
    "Checkpoints are easier with controller-based approach since there are no `_module.` prefixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "privacy_engine.save_checkpoint(\n",
    "    path=\"checkpoint.pt\",\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "print(\"Checkpoint saved!\")\n",
    "\n",
    "# Load checkpoint\n",
    "model_loaded = SimpleClassifier(n_features, 64, n_classes)\n",
    "optimizer_loaded = optim.Adam(model_loaded.parameters(), lr=0.001)\n",
    "\n",
    "privacy_engine_loaded = PrivacyEngineGradSampleController()\n",
    "privacy_engine_loaded.load_checkpoint(\n",
    "    path=\"checkpoint.pt\",\n",
    "    module=model_loaded,\n",
    "    optimizer=optimizer_loaded,\n",
    ")\n",
    "print(\"Checkpoint loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with HuggingFace Transformers\n",
    "\n",
    "The controller-based approach shines with transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run with transformers\n",
    "# from transformers import BertForSequenceClassification\n",
    "# \n",
    "# bert_model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     num_labels=2,\n",
    "# )\n",
    "# \n",
    "# optimizer = optim.AdamW(bert_model.parameters(), lr=5e-5)\n",
    "# \n",
    "# privacy_engine = PrivacyEngineGradSampleController()\n",
    "# bert_model, optimizer, dataloader = privacy_engine.make_private(\n",
    "#     module=bert_model,\n",
    "#     optimizer=optimizer,\n",
    "#     data_loader=your_dataloader,\n",
    "#     noise_multiplier=1.0,\n",
    "#     max_grad_norm=1.0,\n",
    "# )\n",
    "# \n",
    "# # bert_model is still BertForSequenceClassification!\n",
    "# assert isinstance(bert_model, BertForSequenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences Summary\n",
    "\n",
    "| Feature | Standard PrivacyEngine | Controller-Based PrivacyEngine |\n",
    "|---------|------------------------|---------------------------|\n",
    "| Model wrapping | Yes (GradSampleModule) | **No** |\n",
    "| Type preservation | No | **Yes** |\n",
    "| `isinstance()` works | No | **Yes** |\n",
    "| State dict prefixes | `_module.` prefix | **Clean** |\n",
    "| Direct attribute access | Via forwarding | **Direct** |\n",
    "| Transformer compatibility | Can have issues | **Better** |\n",
    "| Requires cleanup | No | **Yes** |\n",
    "| API | Standard | **Same** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Controller-Based?\n",
    "\n",
    "Use `PrivacyEngineGradSampleController` when:\n",
    "- Working with HuggingFace transformers or other models with complex `__getattr__` logic\n",
    "- You need `isinstance()` checks to work correctly\n",
    "- You want clean state dicts without `_module.` prefixes\n",
    "- You need direct access to model attributes\n",
    "\n",
    "Use standard `PrivacyEngine` when:\n",
    "- You have simple models without complex introspection\n",
    "- You don't need the benefits above\n",
    "- You prefer the more battle-tested approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "For more details, see:\n",
    "- [Opacus main documentation](https://opacus.ai)\n",
    "- Other tutorials in the `tutorials/` folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
